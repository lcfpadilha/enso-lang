# Ensō Language - AI Coding Agent Instructions

## Project Overview
**Ensō** is a domain-specific language (DSL) for declarative AI function definitions that compiles to Python. It abstracts away LLM orchestration details, cost tracking, testing with mocks, and multi-model support (OpenAI, Gemini, local), enabling developers to focus on AI prompt engineering rather than integration plumbing.

## Architecture: Three-Layer Stack

### 1. **Frontend: Ensō DSL** (`main.enso`, grammar examples)
- **File**: [compiler.py](compiler.py#L6-L42) (grammar definition)
- **Syntax**: Struct definitions, AI function declarations, test blocks with mocking
- **Key feature**: Functions marked with `ai fn` are compile targets, not regular Python functions

### 2. **Compiler: Transpiler to Python** (`compiler.py`)
- **Parser**: Lark LALR parser consuming the grammar and outputting Python code
- **Two transformers**:
  - `EnsoTransformer`: Converts `.enso` source → executable Python with runtime support
  - `SchemaExtractor`: Analyzes function signatures for introspection (used by SDK generation)
- **Runtime injection**: [RUNTIME_PREAMBLE](compiler.py#L65-L165) is prepended to all compiled output, providing:
  - `EnsoAgent` class (orchestrates LLM calls with cost tracking)
  - LLM drivers (OpenAIDriver, GeminiDriver, LocalDriver)
  - Mock injection system for testing
  - Test runner with `include_ai` flag

### 3. **CLI & Integration** (`enso.py`)
- **Commands**: `init` (create boilerplate), `run`, `test`, `update`
- **Config**: [models.json](models.json) (model registry with pricing) loaded at compile-time
- **Build artifact**: Compiled code written to `__enso_build__/main.py`, executed via subprocess

## Critical Data Flow

```
.enso file → Parser (lark) → EnsoTransformer → RUNTIME_PREAMBLE + Python code
                                               ↓
                                    __enso_build__/main.py (executable)
```

**Model Resolution**: At runtime, `load_model_config()` searches for `models.json` in current dir or `~/.enso/`, enabling offline-first usage.

## Key Patterns & Conventions

### AI Function Declaration
```enso
ai fn analyze(text: String) -> Sentiment {
    instruction: "Analyze sentiment.",
    model: "gpt-4o"
}
```
Compiles to a Python function that:
1. Creates an `EnsoAgent` with the instruction and model
2. Calls `agent.run(input_text, response_model)` → returns `Probabilistic[value, confidence, cost, model_used]`

### Test Syntax with Mocking
```enso
test "Mocked Logic" {
    mock analyze => Sentiment { mood: "Happy", score: 10 };
    let res = analyze("foo");
    assert res.value.score == 10;
}
```
Mocks inject results into `MOCKS` dict; non-AI tests run by default, AI tests require `--real` flag.

### Type System
- **Primitives**: `String`, `Int`, `Float`
- **Complex**: `List<T>`, `Enum<"val1", "val2">` (Enum → Python str)
- **Structs**: Compile to Pydantic `BaseModel` for automatic JSON validation

### Cost Tracking
- `Probabilistic` wraps every AI response with cost metadata
- Cost calculated from token estimates: `(in_toks/1e6 * cost_in) + (out_toks/1e6 * cost_out)`
- Enables billing per-function and per-model analysis

## Developer Workflows

### Build and Run
```bash
enso run main.enso         # Compile → execute
enso test main.enso        # Compile → run test_* functions (mocked)
enso test main.enso --real # Include AI tests (calls actual drivers)
```

### Adding a New Model
1. Update `models.json` with `{name: {type, cost_in, cost_out}}`
2. If new driver type: add `*Driver` class to `RUNTIME_PREAMBLE`, update `get_driver()`
3. Reference in `.enso` file: `model: "new-model-name"`

### Common Edits
- **Grammar changes** → [enso_grammar in compiler.py](compiler.py#L6-L42)
- **Runtime behavior** → [RUNTIME_PREAMBLE](compiler.py#L65-L165) (injected into compiled output)
- **Transformer rules** → [EnsoTransformer class](compiler.py#L169-L208) (map AST nodes → Python)

## Important Gotchas

1. **Grammar is strict**: Trailing commas and field order matter; test thoroughly with `enso test`
2. **Mock system global**: `MOCKS` dict persists; test functions must clear it (done in `run_tests()`)
3. **Cost is estimated**: Token counts calculated as `len(text) // 4`; real usage may vary
4. **Config cascades**: `load_model_config()` searches two paths; first match wins
5. **Schema extraction (SchemaExtractor)** ignores struct and test nodes; only extracts function signatures

## Testing Protocol
- Unit tests in `.enso` files use `test "name" { ... }` syntax
- Mocked tests run by default (no API calls)
- `enso test --real` needed to validate actual LLM integration
- Test failures include mock name for debugging (e.g., `[Mock] Serving response for 'analyze'`)

## Integration Points
- **Input**: `.enso` files (hand-written or generated by SDKs)
- **Output**: Python modules with `run_tests()` and AI functions
- **External deps**: `lark` (parser), `pydantic` (validation), model registry in `models.json`
- **Subprocess boundaries**: Each `enso` command spawns a new Python process for isolation

---
**Last Updated**: 2026-01-02 | **Scope**: MVP (simulated drivers, local config)
